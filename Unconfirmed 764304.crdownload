# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10TafV0NXZ869frBRzJOWp7Zs8ePLwubI
"""

!pip install kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d yasserh/wine-quality-dataset

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

data = pd.read_csv("WineQT.csv")

print(data.describe())

# Correlation heatmap
correlation = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.show()

# Correlations
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True)
plt.title("Correlation Heatmap")
plt.show()

# Pairplot for visualizing relationships
sns.pairplot(data, vars=["alcohol", "pH", "sulphates", "citric acid"], hue="quality")
plt.show()

# Distributions and Histograms
plt.figure(figsize=(10,5 ))
for column in data.columns:
    sns.histplot(data=data, x=column, kde=True, label=column)
plt.legend()
plt.title("Feature Distributions")
plt.show()

# Box Plots
plt.figure(figsize=(12, 6))
for column in data.columns:
    sns.boxplot(data=data, x="quality", y=column)
plt.title("Box Plots by Wine Quality")
plt.xticks(rotation=45)
plt.show()

# Pairplots and Scatter Plots
sns.pairplot(data, hue="quality", markers=["o", "s", "D"])
plt.title("Pairplot by Wine Quality")
plt.show()

# Cluster Analysis (e.g., K-Means clustering)
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42)
data['cluster'] = kmeans.fit_predict(X)
sns.scatterplot(data=data, x='alcohol', y='pH', hue='cluster', palette='coolwarm')
plt.title("Cluster Analysis")
plt.show()

# Separate chemical properties from wine quality
chemical_properties = data.drop("quality", axis=1)
wine_quality = data["quality"]

# Visualize chemical properties by wine quality
for column in chemical_properties.columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=wine_quality, y=chemical_properties[column])
    plt.title(f"{column} vs. Wine Quality")
    plt.xlabel("Wine Quality")
    plt.ylabel(column)
    plt.show()

# Correlations with wine quality
correlations = chemical_properties.corrwith(wine_quality)
correlation_df = pd.DataFrame({'Correlation': correlations})
correlation_df = correlation_df.sort_values(by='Correlation', ascending=False)
print("Correlations with Wine Quality:")
print(correlation_df)

# Heatmap of chemical property correlations
correlation_matrix = chemical_properties.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap of Chemical Properties")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the data into features (X) and target (y)
X = data.drop("quality", axis=1)
y = data["quality"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X, y)
feature_importance = pd.Series(rf.feature_importances_, index=X.columns)
feature_importance.nlargest(5).plot(kind='bar')
plt.title("Top 5 Feature Importances")
plt.show()

X = data.drop("quality", axis=1)
y = data["quality"]

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best model from the grid search
best_clf = grid_search.best_estimator_

# Train the best model on the entire training set
best_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_clf.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Best Model Accuracy: {accuracy * 100:.2f}%")
print("Best Hyperparameters:", grid_search.best_params_)

# Create and train a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

classification_rep = classification_report(y_test, y_pred)
print("Classification Report:\n", classification_rep)

new_wine_sample = np.array([8.2, 0.28, 0.40, 1.8, 0.029, 32.0, 70.0, 0.9968, 3.20, 0.68, 9.6, 5.0]).reshape(1, -1)
scaled_sample = scaler.transform(new_wine_sample)
predicted_quality = model.predict(scaled_sample)

print(f"Predicted Quality: {predicted_quality[0]}")

confusion_mtx = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", confusion_mtx)

models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "SVM": SVC(kernel='linear', C=1),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42)
}

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.get_cmap('Blues'))
plt.title('Confusion Matrix')
plt.colorbar()
plt.xticks(np.arange(5), np.arange(3, 8))
plt.yticks(np.arange(5), np.arange(3, 8))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} Accuracy: {accuracy * 100:.2f}%")

accuracies = []

# Train and evaluate each model
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Create a bar graph to compare model accuracies
plt.figure(figsize=(10, 6))
plt.bar(models.keys(), accuracies)
plt.title("Model Accuracy Comparison")
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.ylim([0, 1.0])  # Set the y-axis range
plt.xticks(rotation=45)
plt.show()

from keras.models import Sequential
from keras.layers import Dense
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import mean_squared_error, r2_score
# Create a neural network model
model = keras.Sequential()
model.add(layers.Input(shape=(X_train.shape[1],)))  # Input layer
model.add(layers.Dense(128, activation='relu'))      # Hidden layer with 128 units and ReLU activation
model.add(layers.Dense(64, activation='relu'))       # Hidden layer with 64 units and ReLU activation
model.add(layers.Dense(1))

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=2)

# Evaluate the model on the test set
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared (R2): {r2}")

# Plot training history (loss over epochs)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Round the predictions to get discrete wine quality values
y_pred = np.round(y_pred)

accuracy = accuracy_score(y_test, y_pred)
print(f"Neural Network Accuracy: {accuracy * 100:.2f}%")

from sklearn.ensemble import VotingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

# Create individual regression models
model1 = DecisionTreeRegressor(random_state=42)
model2 = RandomForestRegressor(n_estimators=100, random_state=42)
model3 = KNeighborsRegressor(n_neighbors=5)

# Create a voting regressor ensemble
ensemble = VotingRegressor(estimators=[('dt', model1), ('rf', model2), ('knn', model3)], weights=[1, 1, 1])

# Train the ensemble model
ensemble.fit(X_train, y_train)

# Make predictions on the test set
y_pred = ensemble.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Voting Regressor MSE: {mse:.2f}")

# Train the individual models
model1.fit(X_train, y_train)
model2.fit(X_train, y_train)
model3.fit(X_train, y_train)

# Make predictions using the individual models
y_pred1 = model1.predict(X_test)
y_pred2 = model2.predict(X_test)
y_pred3 = model3.predict(X_test)

# Calculate Mean Squared Error (MSE) for each model
mse1 = mean_squared_error(y_test, y_pred1)
mse2 = mean_squared_error(y_test, y_pred2)
mse3 = mean_squared_error(y_test, y_pred3)

# Plot the actual vs. predicted wine quality scores for each model
plt.figure(figsize=(10, 6))

plt.subplot(131)
plt.scatter(y_test, y_pred1, alpha=0.5)
plt.xlabel("Actual Quality")
plt.ylabel("Predicted Quality")
plt.title(f"Decision Tree\nMSE: {mse1:.2f}")

plt.subplot(132)
plt.scatter(y_test, y_pred2, alpha=0.5)
plt.xlabel("Actual Quality")
plt.ylabel("Predicted Quality")
plt.title(f"Random Forest\nMSE: {mse2:.2f}")

plt.subplot(133)
plt.scatter(y_test, y_pred3, alpha=0.5)
plt.xlabel("Actual Quality")
plt.ylabel("Predicted Quality")
plt.title(f"K-Nearest Neighbors\nMSE: {mse3:.2f}")

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Create a scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual Wine Quality")
plt.ylabel("Predicted Wine Quality")
plt.title("Actual vs. Predicted Wine Quality (Ensemble)")
plt.show()

from sklearn.linear_model import Ridge, Lasso

# Load your dataset
data = pd.read_csv("WineQT.csv")  #

# Separate the features (X) and the target (y)
X = data.drop("quality", axis=1)
y = data["quality"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize Ridge and Lasso Regression models
ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha parameter
lasso_model = Lasso(alpha=1.0)  # You can adjust the alpha parameter

# Train the Ridge and Lasso models
ridge_model.fit(X_train, y_train)
lasso_model.fit(X_train, y_train)

# Make predictions using Ridge and Lasso models
ridge_pred = ridge_model.predict(X_test)
lasso_pred = lasso_model.predict(X_test)

# Evaluate the models
ridge_mse = mean_squared_error(y_test, ridge_pred)
lasso_mse = mean_squared_error(y_test, lasso_pred)

ridge_r2 = r2_score(y_test, ridge_pred)
lasso_r2 = r2_score(y_test, lasso_pred)

print(f"Ridge MSE: {ridge_mse:.2f}")
print(f"Ridge R-squared: {ridge_r2:.2f}")
print(f"Lasso MSE: {lasso_mse:.2f}")
print(f"Lasso R-squared: {lasso_r2:.2f}")

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.scatter(y_test, ridge_pred, alpha=0.5)
plt.title("Ridge Regression")
plt.xlabel("Actual Quality")
plt.ylabel("Predicted Quality")

plt.subplot(1, 2, 2)
plt.scatter(y_test, lasso_pred, alpha=0.5)
plt.title("Lasso Regression")
plt.xlabel("Actual Quality")
plt.ylabel("Predicted Quality")

plt.tight_layout()
plt.show()

